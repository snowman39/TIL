### 210315

#### 오늘의 개념

* 텍스트 벡터화 : 텍스트를 수치형 텐서로 변환하는 과정으로 여러 가지 방식이 있다
  * 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환
  * 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환
  * 텍스트에서 단어나 문자의 n-그램을 추출하여 각 n-그램을 하나의 벡터로 변환
    * n-그램은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출한다 

* 원-핫 인코딩 : 데이터를 수많은 0과 한개의 1의 값으로 데이터를 구별하는 인코딩이다

* n-그램 : 문장에서 추출한 N개(혹은 그 이하)의 연속된 단어 그룹

* Bow : Bag of words 로 순서 없이 토큰화하여 집합으로 간주하며, 주로 딥러닝 모델보다 얕은 학습 방법의 언어 처리 모델에 사용되는 경향 있다

* 원핫 해싱 : 단어에 명시적으로 인덱스를 할당하고 인덱스를 딕셔너리에 저장하는 대신 단어를 해싱하여 고정된 크기의 벡터로 변환
  * 원핫 인코딩의 변종 중 하나로, 어휘 사전에 있는 고유한 토큰 수가 너무 커서 모두 다루기 어려울 때 사용 
  * 일반적으로 간단한 해싱 함수 사용
  * 명시적인 단어 인덱스가 필요 없어, 메모리를 절야갛고 온라인 방식으로 데이터를 인코딩 할 수 있음
  * 단점은 해시 충돌 : 2개의 단어가 같은 해시를 만들면 머신 러닝 모델은 단어 사이 차이를 인식하지 못한다
  * 해싱 공간 차원이 해싱될 고유 토큰 전체 수 보다 훨씬 크면 충돌 가능성이 감소 된다

* 토큰 임베딩(단어 임베딩) : 저차원의 실수형 벡터
  * 보통 256, 512 차원 또는 큰 어휘 사전의 경우 1024 차원의 단어 임베딩 사용
  * 원핫 인코딩에 비해 더 많은 정보를 더 적은 차원에 저장


#### 오늘의 배운점

* 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 순환 신경망과 1D 컨브넷 두 가지이다
* 텍스트는 단어의 시퀀스나 문자의 시퀀스로 이해할 수 있으며 보통 단어 수준으로 작업하는 경우가 많다
* 컴퓨터 비전이 픽셀에 적용한 패턴 인식인 것 처럼 자연어 처리를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식이다
* 텍스트를 나누는 단위(단어, 문자, n-그램)을 토큰이라고 하고, 텍스트를 토큰으로 나누는 과정을 토큰화라고 한다
* 모든 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것으로 이루어진다
* n-그램은 로지스틱 회귀나 랜덤 포레스트 같은 얕은 학습 방법 텍스트 처리 모델에서 강력하고 유용한 특성 공학 기법이다
* 단어 임베딩을 만드는 방법은 두 가지
  * (문서 분류나 감성 예측 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습한다. 이 경우 랜덤한 단어 벡터로 시작해서 신경망 가중치를 학습하는 것과 같은 방식으로 단어 벡터 학습
  * 풀려는 문제가 아닌 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩 로드, 이를 사전 훈련된 단어 임베딩이라 한다
