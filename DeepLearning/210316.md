### 210316

#### 오늘의 개념

* Word2vec : 구글의 토마스 미코로프가 개발한, 가장 유명하고 성공적인 단어 임베딩 방법 중 하나
* Glove : 스탠포드 대학의 연구자들이 개발한 임베딩 기법으로, 단어의 동시 출현 통계를 기록한 행렬을 분해하는 기법 사용

* 피드포워드 네트워크 : 메모리가 없어 입력 간에 유지되는 상태가 없는 네트워크

* 순환 신경망 : 시퀀스의 원소를 순회하면서 처리한 정보를 상태에 저장해두는 신경망

* LSTM층 : SimpleRNN 의 변종으로, 나중을 위해 정보를 저장해두며 처리 과정에서 오래된 시그널이 점차 소실되는 것을 막아준다그래디언트 소실 문제)

#### 오늘의 배운점

* 단어와 밀집 벡터를 연관 짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것이다
  * 단 이는, 임베딩 공간이 구조적이지 않게 된다 ( 예 : accurate 와 exact 는 비슷한 의미이지만 완전히 다른 임베딩을 갖는다 ) 
* 단어 벡터 사이에 좀 더 추상적이고 기하학적인 관계를 얻으려면 단어 사시에 있는 의미 관계를 반영해야 한다
* 일반적으로 두 단어 벡터 사이의 거리는 단어 사이의 의미 거리와 관계되어 있다
* 임베딩 공간의 특정 방향도 의미를 가질 수 있다
* 완전 연결 네트워크나 컨브넷은 메모리가 없었다. 네트워크에 주입되는 입력은 개별적으로 처리되며 입력 간에 유지되는 상태가 없다
  * 이런 네트워크로 시퀀스나 시계열 데이터 포인트를 처리하려면 네트워크에 전체 시퀀스를 주입해야한다
  * 즉, 전체 시퀀스를 하나의 데이터 포인트로 변환해야 한다

* 간단하고 쉽게 만들 수 있는 모델을 통해서 상식 수준의 기준점을 세워놓은 다음, 더 복잡한 모델을 만들면 좋다

* 학습 알고리즘이 특정 종류의 간단한 모델을 찾도록 코딩되어 있지 않다면, 모델 파라미터를 학습하는 방법은 간단한 문제를 위해 간단한 해결책을 찾지 못할 수 있다
