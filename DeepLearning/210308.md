### 210308

#### 학습할 책 : 케라스 창시자에게 배우는 딥러닝

- 선정 이유 : 딥러닝 모델을 구성할 때 자주 사용되는 케라스 라이브러리를 이용해 빠르게 예제를 풀어보고, 딥러닝에 전반을 훑어본다

#### 오늘의 개념

* 인공 지능 : 사람이 수행하는 지능적인 작업을 자동화하는 활동
  * 머신 러닝을 포괄하는 종합적인 분야
  * 학습 과정이 없는 방법들도 포함 (ex: 명시적인 규칙으로 지식을 다루는 접근 방법인 심볼릭 AI)

* 머신 러닝 : 샘플과 기댓값이 주어졌을 때 데이터 처리 작업을 위한 실행 규칙을 찾는 활동
  * 명시적으로 프로그램되는 것이 아닌 훈련되는 방식
  * 작업과 관련된 많은 샘플을 제공하면, 데이터에서 통계적 구조를 찾아 이를 자동화하기 위한 규칙을 만들어 낸다
  * 고성능 하드웨어와 대량의 데이터셋이 가능해지면서 각광받기 시작
  * 데이터셋이 크고 복잡해 베이지안 분석 같은 전통적 통계분석 방법은 현실적으로 적용하기 어려움
  * 세 가지 필수 요소 : 입력 데이터 포인트, 기대 출력, 알고리즘 성능을 측정하는 방법
  
* 딥러닝 : 머신 러닝의 특정한 분야로, 연속된 층에서 점진적으로 의미 있는 표현을 학습하는 방식
  * 층을 겹겹이 쌓아 올려 구성한 신경망이라는 모델을 사용

* 손실함수 : 신경망이 얼마나 잘 예측했는지 측정하는 함수, 신경망의 예측과 타깃의 차이를 점수로 계산

* 옵티마이저 : 손실 점수를 피드백 신호로 사용해 가중치 값을 수정, 역전파 알고리즘으로 구현되어 있음 

* 과대적합 : 머신 러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향

* 텐서 : 데이터를 위한 컨테이너, 일반적으로 머신 러닝 시스템에서는 텐서를 기본 데이터 구조로 사용
  * 벡터 데이터: (samples, features) 2D 텐서
  * 시계열 데이터 : (samples, timesteps, features) 3D 텐서
  * 이미지 : (samples, height, width, channels) 4D 텐서
  * 동영상 : (samples, frames, height, width, channels) 5D 텐서

* 활성화함수 : 비선형성 함수
  * 이게 없다면 Dense 층은 선형적인 연산인 점곱과 덧셈 2개로 구성됨
  * 즉, 입력에 대한 선형 변환만을 학습할 수 있으며, 이런 가설 공간은 제약이 많고 층을 여러 개로 구성하는 장점이 사라짐
  * 가설 공간을 풍부히 만들어 층을 깊게 만드는 장점을 살리기 위해서는 활성화함수를 추가해야 

* relu : 활성화함수의 일종으로 relu(x) = max(x,0) 이다. 
* softmax : 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수

* 브로드캐스팅 : 크기가 다른 두 텐서가 더해질 때 시행됨
  * 큰 텐서의 ndim 에 맞게 작은 텐서의 축이 추가됨
  * 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복  

* epoch : 전체 훈련 데이터에 수행되는 반복 횟수

* 은닉 유닛 : 하나의 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 된다.

* K-겹 검증 : 데이터 포인트가 적은 경우, 데이터를 K개의 분할로 나누고 K 개의 모델을 만들어 K-1 개의 분할에서 훈련하고 나머지 분할에서 평가하는 

#### 오늘의 예제

##### 영화 리뷰 분류 ( 이진 분류 )

데이터셋 : 인터넷 영화 데이터베이스(IMDB)

```python

from keras.datasets import imdb
import numpy as np
from keras import models
from keras import layers
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

model = models.Sequential()
model.add(layers.Dense(16,activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16,activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

# history = model.fit(partial_x_train,
#                     partial_y_train,
#                     epochs=20,
#                     batch_size=512,
#                     validation_data=(x_val, y_val))

# history_dict = history.history
# loss = history_dict['loss']
# val_loss = history_dict['val_loss']

# epochs = range(1, len(loss) + 1)

# plt.plot(epochs, loss, 'bo', label='Training loss')
# plt.plot(epochs, val_loss, 'b', label='Validation loss')
# plt.title('Training and validation loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()

# plt.show()

# plt.clf()
# acc = history_dict['acc']
# val_acc = history_dict['val_acc']

# plt.plot(epochs, acc, 'bo', label='Training acc')
# plt.plot(epochs, val_acc, 'b', label='Validation acc')
# plt.title('Training and validation accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()

# plt.show()

history = model.fit(x_train,
                    y_train,
                    epochs=4,
                    batch_size=512)
results = model.evaluate(x_test,y_test)

model.predict(x_test)

```

훈련 결과
<img width="400" alt="스크린샷 2021-03-08 오후 3 16 27" src="https://user-images.githubusercontent.com/48688233/110303465-c2883980-803d-11eb-9faf-443fe4c05b72.png">
<img width="400" alt="스크린샷 2021-03-08 오후 3 16 10" src="https://user-images.githubusercontent.com/48688233/110303478-c61bc080-803d-11eb-99bb-5d4d55015962.png">
<img width="357" alt="스크린샷 2021-03-08 오후 3 22 18" src="https://user-images.githubusercontent.com/48688233/110303526-d338af80-803d-11eb-85e1-f5d0eaa1f887.png">

* 훈련 손실은 에포크마다 감소하고 훈련 정확도는 에포크마다 증가한다 (경사 하강법 최적화를 사용했을 때 기대한 바)
* 단, 검증 손실과 정확도는 4 번째 에포크 이후에서 역전됨 (과대적합)
* 두 번째 에포크부터 훈련데이터에 과도하게 최적화되어 이에 특화 표현을 학습해 일반화되지 못한 것
* 에포크 수를 줄여서 확인해야 함, 88% 정확도 달성

배운 내용
* 원본 데이터를 신경망에 텐서로 주입하기 위해서는 많은 전처리가 필요하다
* 입력 데이터가 벡터고 리에블은 스칼라 (1 or 0) 인 상황에서 잘 작동하는 네트워크 종류는 relu 활성화 함수를 사용한 완전 연결 층을 쌓은 것
* Dense 층에 전달한 매개변수는 은닉 유닛의 개수이다
* 은닉 유닛을 늘리면 신경망이 더욱 복잡한 표현을 학습할 수 있지만, 계산 비용이 커지고 원치 않는 패턴을 학습할 수 있다 (과대적합)
* 신경망의 출력이 확률일 때는 손실함수로서 crossentropy 가 좋은 선택이다 (이진 분류에서는 binary_crossentropy)
* 이진 분류 문제에서 네트워크는 하나의 유닛과 sigmoid 활성화 함수를 가진 Dense 층으로 끝나야 한다 (출력 : 0 ~ 1 사이의 스칼라 값)
* rmsprop 옵티마이저는 일반적으로 충분히 좋은 선택지
* 신경망의 과대적합이 일어날 수 있으니, 항상 훈련 세트 이외의 데이터에서 성능을 모니터링 해야함

##### 뉴스 기사 분류 (다중 분류)

데이터셋 : 로이터 데이터셋 (reuter)


```python

from keras.datasets import reuters
import numpy as np
from keras.utils.np_utils import to_categorical
from keras import models
from keras import layers
import matplotlib.pyplot as plt

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])

def vectorize_sequences(sequences, dimension=10000):
  results = np.zeros((len(sequences), dimension))
  for i, sequence in enumerate(sequences):
    results[i, sequence] = 1.
  return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

def to_one_hot(labels, dimension=46):
  results = np.zeros((len(labels), dimension))
  for i, label in enumerate(labels):
    results[i, label] = 1.
  return results

one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)

one_hot_test_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
# model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

x_val = x_train[:1000]
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]

# history = model.fit(partial_x_train,
#                     partial_y_train,
#                     epochs=20,
#                     batch_size=512,
#                     validation_data=(x_val, y_val))

# loss = history.history['loss']
# val_loss = history.history['val_loss']

# epochs = range(1, len(loss)+1)
# plt.plot(epochs, loss, 'bo', label='Training loss')
# plt.plot(epochs, val_loss, 'b', label='Validation loss')
# plt.title('Training and validation loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()

# plt.show()

# plt.clf()

# acc = history.history['accuracy']
# val_acc = history.history['val_accuracy']

# plt.plot(epochs, acc, 'bo', label='Training acc')
# plt.plot(epochs, val_acc, 'b', label='Validation acc')
# plt.title('Training and validation accuracy')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()

# plt.show()

model.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)

predictions = model.predict(x_test)

```

훈련 결과

<img width="400" alt="스크린샷 2021-03-08 오후 3 44 14" src="https://user-images.githubusercontent.com/48688233/110304583-1cd5ca00-803f-11eb-9f2e-17cddeac56bd.png">
<img width="400" alt="스크린샷 2021-03-08 오후 3 44 23" src="https://user-images.githubusercontent.com/48688233/110304593-1f382400-803f-11eb-89f6-79a2d49ae217.png">
<img width="349" alt="스크린샷 2021-03-08 오후 3 47 18" src="https://user-images.githubusercontent.com/48688233/110304602-219a7e00-803f-11eb-9f02-e73545d9ddb5.png">

* 9번째 에포크 이후에서 과대적합 시작됨
* 에포크 수를 줄여서 확인한 결과 78% 정확도 달성

배운 내용
* 레이블이 범주형 인코딩 되어있을 경우에는 손실함수로 categorical_crossentropy, 정수 레이블일 경우는 sparse_categorical_crossentropy 사용
* 중간층의 히든 유닛이 마지막 출력 차원수보다 많이 적으면 정보에 병목이 생긴다
* N개의 클래스로 데이터 포인트를 분류하려면 네트워크의 마지막 Dense 층의 크기는 N이어야 한다
* 단일 레이블, 다중 분류 문제에서 N개의 클래스에 대한 확률 분포를 출력하기 위해서는 softmax 활성화함수 사용해야 한다

##### 주택 가격 예측 (회귀)

회귀 : 개별적인 레이블이 아닌 연속적인 값을 예측하는 것

```python

from keras.datasets import boston_housing
from keras import models
from keras import layers
import numpy as np
import matplotlib.pyplot as plt

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std

test_data -= mean
test_data /= std

def build_model():
  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu',
                         input_shape=(train_data.shape[1],)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(1))
  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
  return model

k = 4
num_val_samples = len(train_data) // k
num_epochs = 500
all_mae_histories = []
# all_scores = []
for i in range(k):
  print('처리중인 폴드 #', i)
  val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]

  partial_train_data = np.concatenate(
      [train_data[:i * num_val_samples],
       train_data[(i+1) * num_val_samples:]],
       axis=0
  )
  partial_train_targets = np.concatenate(
      [train_targets[:i * num_val_samples],
       train_targets[(i+1) * num_val_samples:]],
       axis=0
  )

  model = build_model()
  # model.fit(partial_train_data, partial_train_targets,
  #           epochs=num_epochs, batch_size=1, verbose=0)
  # val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
  # all_scores.append(val_mae)
  history = model.fit(partial_train_data, partial_train_targets,
                      validation_data =(val_data, val_targets),
                      epochs=num_epochs, batch_size=1, verbose=0)
  mae_history = history.history['val_mae']
  all_mae_histories.append(mae_history)

average_mae_history = [
                       np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)
]

def smooth_curve(points, factor=0.9):
  smoothed_points = []
  for point in points:
    if smoothed_points:
      previous = smoothed_points[-1]
      smoothed_points.append(previous * factor + point * (1 - factor))
    else:
      smoothed_points.append(point)
  return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) +1), smooth_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()

```

훈련 결과
<img width="400" alt="스크린샷 2021-03-08 오후 7 01 10" src="https://user-images.githubusercontent.com/48688233/110306005-ac2fad00-8040-11eb-9a30-9573f25e6d25.png">
* 80번째 에포크 이후에서 과대적합 시작됨

배운 내용
* 상이한 스케일을 가진 값을 신경망에 주입하면 학습을 어렵게 만들 수 있기에, 특성별로 정규화를 해주어야 한다
* 테스트 데이터를 정규화할 때도 훈련 데이터에서 계산한 값으로 해주어야 한다 
* 훈련 데이터 개수가 적을수록 과대적합이 더 쉽게 일어나기에, 이를 피하기 위해 작은 네트워크를 구성하여 사용한다
* mse(mean squared error)는 회귀 문제에서 널리 사용되는 손실 함수이다
* 회귀에서 사용되는 평가 지표는 분류(accuracy)에서와 달리 mae(mean absolute error)를 사용한다

느낀점 : 모델을 짜는 코드를 직접 손으로 따라 쳐보니, 제법 익숙해진 것 같다

더 해보면 좋을 것 : 교재에서 제시한 모델이 아니라 다른 방식으로 모델을 구성해서 정확도 비교해보기

좀 더 알아보면 좋을 개념 : 2D: 완전 연결층 & 밀집 연결 층, 3D: 순환 층, 4D: 2D 합성곱 층, 원핫인코딩, 원-핫 벡터
