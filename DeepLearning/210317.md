### 210317

#### 오늘의 개념

* GRU 층 : LSTM과 같은 원리로 동작하지만 조금 더 간결하고, 계산 비용이 덜 든다 (단, LSTM 만큼 표현 학습 능력이 좋지 않을 수 있다)

* 양방향 RNN : RNN이 순서나 타임스텝에 민감하다는 성질을 이용해, GRU나 LSTM 같은 RNN 2개를 사용해서, 각 RNN은 입력 시퀀스를 서로 다른 방향으로 처리해서 각 표현을 합친다
  * 시퀀스르 양쪽 방향으로 처리하므로 단방향 RNN이 놓치기 쉬운 패턴을 감지할 수 있다
  * 자연어 처리 문제에 유용하다
  * 최근 정보가 오래된 것보다 훨씬 의미 있는 시퀀스(예 : 기온 예측) 데이터에서는 잘 작동하지 않을 수 있다

* 앙상블 : 여러 단순한 모델을 결합하여 정확한 모델을 만드는 방법

#### 오늘의 배운점

* 순환 네트워크에 적절하게 드롭아웃을 사용하는 방법은 타임스텝마다 랜덤하게 드롭아웃 마스크를 바꾸는 것이 아니라 동일한 드롭아웃 마스크를 모든 타임스텝에 적용해야 한다
  * 타임스텝마다 랜덤한 드롭아웃 마스크를 적용하면 오차 신호가 전파되는 것을 방해하고 학습 과정에 해를 끼친다
* 드롭아웃으로 규제된 네트워크는 언제나 완전히 수렴하는데 더 오래 걸린다

* 늘 그렇듯이 딥러닝은 과학보다는 예술에 가깝습니다 (p.299)

* 1D 컨브넷은 전형적으로 팽창된 커널과 함께 사용되며, 특정 시퀀스 처리 문제에서 RNN과 견줄만 하다
  * 오디오 생성과 기계 번역 분야에서 큰 성공 거둠
  * 일반적으로 RNN 보다 계산 비용이 훨씬 싸다

* 1D 컨브넷을 RNN 이전에 전처리 단계로 사용하면, 컨브넷의 속도와 경량함을 RNN의 순서 감지 능력과 결합할 수 있다
