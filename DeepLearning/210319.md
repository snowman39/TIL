### 210319

#### 오늘의 개념

* 인셉션 : 네트워크 안의 네트워크 구조 에서 영감을 받아 세게디와 구글 동료들이 만든 구조
  * 합성곱 신경망에서 인기 있는 네트워크 구조이다
  * 나란히 분리된 가지를 따라 모듈을 쌓아 독립된 작은 네트워크 처럼 구성한다
  * 네트워크가 따로따로 공간 특성과 채널 방향의 특성을 학습하도록 도와, 한꺼번에 학습하는 것보다 효과가 더 좊다

* 엑셉션 : 인셉션에서 일부 영감을 얻어, 채널 방향의 학습과 공간 방향의 학습을 극단적으로 분리한다는 아이디어에 착안하여, 인셉션 모듈을 깊이별 분리 합성곱으로 바꾼다
  * 깊이별 합성곱 다음 점별 합성곱이 뒤따른다 
  * 극단적인 인셉션

* 잔차 연결 : 그래프 형태의 네트워크 컴포넌트로 그래디언트 소실과 표현 병목을 해결함
  * 일반적으로 10개 층 이상을 가진 모델에 잔차 연결을 추가하면 도움이 된다
  * 하위 층의 출력을 상위 층의 입력으로 사용한다

* 표현 병목 : Sequential 모델에서 층의 학습은 이전 층의 활성화 출력 정보만 사용하면서 발생하는 문제로, 특정 층이 너무 작으면 이 활성화 출력에 얼마나 많은 정보를 채울 수 있느냐에 따라 모델 성능이 좌우된다
  * 손실된 정보는 영구 불변이다 

* 그래디언트 소실 : 피드백 신호가 깊이 쌓인 층을 통과하여 전파되면 신호가 아주 작거나 완전히 사라질 수도 있게 되는데, 이 경우 네트워크가 훈련되지 않는다
  * 심층 신경망과 순환 신경망 모두에서 나타난다 (모두 피드백 신호가 일련의 긴 연산을 통과하여 전파되기 때문)

#### 오늘의 배운점

* 최근에 개발된 많은 신경망 구조는 선형적이지 않은 네트워크 토폴로지가 필요하다
* 여러 경우 다중 입력 모델, 다중 출력 모델, 그래프 구조를 띤 모델이 필요할 때는 케라스의 Sequential 클래스로는 만들지 못하므로 함수형 API를 사용한다
* 다중 출력 모델의 경우 다른 손실 함수를 지정하게 되는데, 각각의 손실 값이 불균형하면 치우친 표현을 최적화할 수 있기 때문에, 임의대로 각각에 가중치를 부여할 수 있다
